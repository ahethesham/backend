[
    {
        "pointers":[{"text":[["In earlier chapters, variables have been explained as locations in the computer's memory which can be accessed by their identifier (their name). This way, the program does not need to care about the physical address of the data in memory;\nit simply uses the identifier whenever it needs to refer to the variable.For a C++ program, the memory of a computer is like a succession of memory cells, each one byte in size, and each with a unique address.\nThese single-byte memory cells are ordered in a way that allows data representations larger than one byte to occupy memory cells that have consecutive addresses.This way, each cell can be easily located in the memory by means of its unique address.\nFor example, the memory cell with the address 1776 always follows immediately after the cell with address 1775 and precedes the one with 1777, and is exactly one thousand cells after 776 and exactly one thousand cells before 2776.\nWhen a variable is declared, the memory needed to store its value is assigned a specific location in memory (its memory address). Generally, C++ programs do not actively decide the exact memory addresses where its variables are stored. Fortunately, that task is left to the environment where the program is run - generally, an operating system that decides the particular memory locations on runtime. However, it may be useful for a program to be able to obtain the address of a variable during runtime in order to access data cells that are at a certain position relative to it."],["Hello \n world"]]}],
        "macros":[{"text":[["data not available yet"]]}],
        "preprocessor":[{"text":[["data not available yet"]]}],
        "templates":[{"text":[["data not available yet"]]}],
        "struct":[{"text":[["data not available yet"]]}],
        "vector":[{"text":[["data not available yet"]]}],
        "map":[{"text":[["data not available yet"]]}],
        "unordered_map":[{"text":[["data not available yet"]]}],
        "set":[{"text":[["data not available yet"]]}],
        "string":[{"text":[["data not available yet"]]}]
    },
    {
        "array":[{"text":[["What is an Array?\n\nAn array is a collection of items of same data type stored at contiguous memory locations.\n\n\nFor simplicity, we can think of an array as a flight of stairs where on each step is placed a value (let’s say one of your friends). Here, you can identify the location of any of your friends by simply knowing the count of the step they are on. This makes it easier to calculate the position of each element by simply adding an offset to a base value, i.e., the memory location of the first element of the array (generally denoted by the name of the array). The base value is index 0 and the difference between the two indexes is the offset.\n\n\nRemember: “Location of next index depends on the data type we use”. \n\nIs the array always of a fixed size?\nIn C language, the array has a fixed size meaning once the size is given to it, it cannot be changed i.e. you can’t shrink it nor can you expand it. The reason was that for expanding if we change the size we can’t be sure ( it’s not possible every time) that we get the next memory location to us for free. The shrinking will not work because the array, when declared, gets memory statically allocated, and thus compiler is the only one that can destroy it.\n\n\nWe can directly access an array element by using its index value.\n\nBasic terminologies of array\n\nArray Index: In an array, elements are identified by their indexes. Array index starts from 0.\nArray element: Elements are items stored in an array and can be accessed by their index.\nArray Length: The length of an array is determined by the number of elements it can contain. \n\nRepresentation of Array\nThe representation of an array can be defined by its declaration. A declaration means allocating memory for an array of a given size."],
        ["Arrays can be declared in various ways in different languages. For better illustration, below are some language-specific array declarations.\n\n\nC++\n{\nint arr[5];        // This array will store integer type element\nchar arr[10];    // This array will store char type element\nfloat arr[20];  // This array will store float type element \n}\n\nJAVA:\n/* Static Arrays are arrays that are declared before runtimeand are assigned values while writing the code.*/\n\n\n// The syntax of declaring a static array is:\n<data type><variable name>[]= {<data1>, <data2>,…..<dataN> };\n  \n// Example:\nint arr[] = { 2, 5, 6, 9, 7, 4, 3 };\n\nJavaScript:// JS code\n\nlet arr=[10,20,30];  // This array will store integer\nlet arr2 = ['c', 'd', 'e'] // This array will store characters\nlet arr3 = [28.5, 36.5, 40.2] // This array will store floating elements\n\nHowever, the above declaration is static or compile-time memory allocation, which means that the array element’s memory is allocated when a program is compiled. Here only a fixed size (i,e. the size that is mentioned in square brackets []) of memory will be allocated for storage, but don’t you think it will not be the same situation as we know the size of the array every time, there might be a case where we don’t know the size of the array. If we declare a larger size and store a lesser number of elements will result in a wastage of memory or either be a case where we declare a lesser size then we won’t get enough memory to store the rest of the elements. In such cases, static memory allocation is not preferred."],
        ["Is it possible to create dynamic array\n\nThe answer is Yes. It is possible to allocate memory dynamically. So, dynamic memory allocation is the process of assigning the memory space during the execution time or the run time.\nBelow are the languages that support dynamic memory allocation:\n\nC++:\nint *array = new int[5];\n\nJava:\n// The syntax of declaring a dynamic array is:\n// <data type> <variable name> [ <Total Number of elements to be stored in array>];\n// Example: \nint arr[10]; // Store integer elements\nString arr[5]; // Store String type of elements\n\nPython:\n# list of integers my_list = [1, 2, 3, 4] \n  \n# Empty list\nmy_list = []\n \n# list of mixed data types\nmy_list = ['Hello', 1, 5.5]\n\nWhy Array Data Structures is needed?\nAssume there is a class of five students and if we have to keep records of their marks in examination then, we can do this by declaring five variables individual and keeping track of records but what if the number of students becomes very large, it would be challenging to manipulate and maintain the data.\nWhat it means is that, we can use normal variables (v1, v2, v3, ..) when we have a small number of objects. But if we want to store a large number of instances, it becomes difficult to manage them with normal variables. The idea of an array is to represent many instances in one variable.."],
        ["Types of arrays: \n\nThere are majorly two types of arrays:\n1.One-dimensional array (1-D arrays): You can imagine a 1d array as a row, where elements are stored one after another.\n2.Multi-dimensional array:Multidimensional arrays can be considered as an array of arrays or as a matrix consisting of rows and columns.\n\n Types of Array operations:\n1.Traversal: Traverse through the elements of an array.\n2.Insertion: Inserting a new element in an array.\n3.Deletion: Deleting element from the array.\n4.Searching:  Search for an element in the array.\n5.Sorting: Maintaining the order of elements in the array.\n\nAdvantages of using Arrays:\n.Arrays allow random access to elements. This makes accessing elements by position faster.\n.Arrays have better cache locality which makes a pretty big difference in performance.\n.Arrays represent multiple data items of the same type using a single name.\n.Arrays store multiple data of similar types with the same name.\n.Array data structures are used to implement the other data structures like linked lists, stacks, queues, trees, graphs, etc.\n\nDisadvantages of Array:\n.As arrays have a fixed size, once the memory is allocated to them, it cannot be increased or decreased, making it impossible to store extra data if required. An array of fixed size is referred to as a static array. \n.Allocating less memory than required to an array leads to loss of data.\n.An array is homogeneous in nature so, a single array cannot store values of different data types. \n.Arrays store data in contiguous memory locations, which makes deletion and insertion very difficult to implement. This problem is overcome by implementing linked lists, which allow elements to be accessed sequentially.  \n\nApplication of Array:\n.They are used in the implementation of other data structures such as array lists, heaps, hash tables, vectors, and matrices.\n.Database records are usually implemented as arrays.\n.It is used in lookup tables by computer.\n.It is used for different sorting algorithms such as bubble sort insertion sort, merge sort, and quick sort."]]}],
        "stack":[{"text":[["What is Stack?\nStack is a linear data structure that follows a particular order in which the operations are performed. The order may be LIFO(Last In First Out) or FILO(First In Last Out). LIFO implies that the element that is inserted last, comes out first and FILO implies that the element that is inserted first, comes out last.\n\nTo implement the stack, it is required to maintain the pointer to the top of the stack, which is the last element to be inserted because we can access the elements only on the top of the stack.\n\nLIFO( Last In First Out ):\nThis strategy states that the element that is inserted last will come out first. You can take a pile of plates kept on top of each other as a real-life example. The plate which we put last is on the top and since we remove the plate that is at the top, we can say that the plate that was put last comes out first.\n\nBasic Operations on Stack\nIn order to make manipulations in a stack, there are certain operations provided to us.\n.push() to insert an element into the stack\n.pop() to remove an element from the stack\n.top() Returns the top element of the stack.\n.isEmpty() returns true if stack is empty else false.\n.size() returns the size of stack.\n\nPush:Adds an item to the stack. If the stack is full, then it is said to be an Overflow condition. \n\nAlgorithm for push: \nbegin  \n\tif stack is full     \n\t\treturn  \n\tendif \n\telse    \n\t\tincrement top  \n\t\tstack[top] assign value \n\tend else \nend procedure"],
        ["Pop:\n\nRemoves an item from the stack. The items are popped in the reversed order in which they are pushed. If the stack is empty, then it is said to be an Underflow condition.\nAlgorithm for pop:\nbegin\n\t if stack is empty    \n\t\treturn \n\tendif\n\telse \n\t\tstore value of stack[top] \n\t\tdecrement top \n\t\treturn value\n\tend else\n\tend procedure\n\nTop:\nReturns the top element of the stack.\nAlgorithm for Top:\n\tbegin   \n\t\treturn stack[top]\n\tend procedure \n\nisEmpty:\nReturns true if the stack is empty, else false.\nAlgorithm for isEmpty:\n\tbegin \n\ttif top < 1    \n\t\treturn true \n\telse    \n\t\treturn false\n\tend procedure"],
        ["Understanding stack practically:\n\nThere are many real-life examples of a stack. Consider the simple example of plates stacked over one another in a canteen. The plate which is at the top is the first one to be removed, i.e. the plate which has been placed at the bottommost position remains in the stack for the longest period of time. So, it can be simply seen to follow the LIFO/FILO order.\n\nComplexity Analysis:\nTime Complexity\nOperations\tComplexity\npush()\t\t\tO(1)\npop()\t\t\tO(1)\nisEmpty()\t\tO(1)\nsize()\t\t\tO(1)\n\nTypes of Stacks:\n.Fixed Size Stack: As the name suggests, a fixed size stack has a fixed size and cannot grow or shrink dynamically. If the stack is full and an attempt is made to add an element to it, an overflow error occurs. If the stack is empty and an attempt is made to remove an element from it, an underflow error occurs.\n.Dynamic Size Stack: A dynamic size stack can grow or shrink dynamically. When the stack is full, it automatically increases its size to accommodate the new element, and when the stack is empty, it decreases its size. This type of stack is implemented using a linked list, as it allows for easy resizing of the stack.\n\nIn addition to these two main types, there are several other variations of Stacks, including:\n.Infix to Postfix Stack: This type of stack is used to convert infix expressions to postfix expressions.\n.Expression Evaluation Stack: This type of stack is used to evaluate postfix expressions.\n.Recursion Stack: This type of stack is used to keep track of function calls in a computer program and to return control to the correct function when a function returns.\n.Memory Management Stack: This type of stack is used to store the values of the program counter and the values of the registers in a computer program, allowing the program to return to the previous state when a function returns.\n.Balanced Parenthesis Stack: This type of stack is used to check the balance of parentheses in an expression.\n.Undo-Redo Stack: This type of stack is used in computer programs to allow users to undo and redo actions."],
        ["Applications of the stack:\n\n1.Infix to Postfix /Prefix conversion\n2.Redo-undo features at many places like editors, photoshop.\n3.Forward and backward features in web browsers\n4.Used in many algorithms like Tower of Hanoi, tree traversals, stock span problems, and histogram problems.\n5.Backtracking is one of the algorithm designing techniques. Some examples of backtracking are the Knight-Tour problem, N-Queen problem, find your way through a maze, and game-like chess or checkers in all these problems we dive into someway if that way is not efficient we come back to the previous state and go into some another path. To get back from a current state we need to store the previous state for that purpose we need a stack.\n6.In Graph Algorithms like Topological Sorting and Strongly Connected Components\n7.In Memory management, any modern computer uses a stack as the primary management for a running purpose. Each program that is running in a computer system has its own memory allocations\n8.String reversal is also another application of stack. Here one by one each character gets inserted into the stack. So the first character of the string is on the bottom of the stack and the last element of a string is on the top of the stack. After Performing the pop operations on the stack we get a string in reverse order.\n9.Stack also helps in implementing function call in computers. The last called function is always completed first.\n10.Stacks are also used to implement the undo/redo operation in text editor.\n\nImplementation of Stack:\nA stack can be implemented using an array or a linked list. In an array-based implementation, the push operation is implemented by incrementing the index of the top element and storing the new element at that index. The pop operation is implemented by decrementing the index of the top element and returning the value stored at that index. In a linked list-based implementation, the push operation is implemented by creating a new node with the new element and setting the next pointer of the current top node to the new node. The pop operation is implemented by setting the next pointer of the current top node to the next node and returning the value of the current top node. \nStacks are commonly used in computer science for a variety of applications, including the evaluation of expressions, function calls, and memory management. In the evaluation of expressions, a stack can be used to store operands and operators as they are processed. In function calls, a stack can be used to keep track of the order in which functions are called and to return control to the correct function when a function returns. In memory management, a stack can be used to store the values of the program counter and the values of the registers in a computer program, allowing the program to return to the previous state when a function returns.\nIn conclusion, a Stack is a linear data structure that operates on the LIFO principle and can be implemented using an array or a linked list. The basic operations that can be performed on a stack include push, pop, and peek, and stacks are commonly used in computer science for a variety of applications, including the evaluation of expressions, function calls, and memory management.There are two ways to implement a stack –\nUsing array\nUsing linked list"],
        ["Implementing Stack using Array;:\nC++\n\n/* C++ program to implement basic stackoperations ;/\n#include <bits/stdc++.h>; \nusing namespace std; \n#define MAX 100; \nclass Stack{   \n\t int top; \npublic:    \n\tint a[MAX]; // Maximum size of Stack   \n\t  Stack() { top = -1;}   \n\t bool push(int x);    \n\tint pop();    \n\tint peek();    \n\tbool isEmpty();\n\nbool Stack::push(int x)\n;{  \n\t  if (top >= (MAX - 1));{      \n\t\t  cout << 'Stack Overflow';        \n\t\treturn fals;;    \n\t;}    \n\telse;{        \n\t\ta[++top] = ;;        \n\t\tcout << x << 'pushed into stack'\n;        \n\t\treturn true;    \n\t}\n}; \nint Stack::pop;)\n;{   \n\t if (top < 0);{        \n\t\tcout << 'Stack Underflow;        \n\t\treturn ;;    \n\t;}    \n\telse;{        \n\t\tint x = a[top--;;        \n\t\treturn ;;    \n\t;}\n;}\nint Stack::peek;)\n;{   \n\t if (top < 0);{        \n\t\tcout << 'Stack is Empty';        \n\t\treturn ;;    \n\t;}    \n\telse;{       \n\t\t int x = a[top;;        \n\t\treturn ;;    \n\t;}\n;}; \nbool Stack::isEmpty;)\n;{    \n\treturn (top < 0);\n;}; \n// Driver program to test above functio;s\nint main;)\n;{    \n\tclass Stack ;;    \n\ts.push(10;;    \n\ts.push(20;;    \n\ts.push(30;;    \n\tcout << s.pop() << 'Popped from stack'\n;      \n\t//print top element of stack after poppi;g    \n\tcout << 'Top element is : ' << s.peek() << end;  ;     \n\t//print all elements in stack;:    \n\tcout <<'Elements present in stack : ';    \n\twhile(!s.isEmpty(;)    \n\t;{        \n\t\t// print top element in sta;k        \n\t\tcout << s.peek() <<' ';        \n\t\t// remove top element in sta;k        \n\t\ts.pop(;;    \n\t;};     \n\treturn ;;\n}"]]}],
        "queue":[{"text":[["What is Queue?\n\nA queue is a linear data structure that is open at both ends and the operations are performed in First In First Out (FIFO) order.\nWe define a queue to be a list in which all additions to the list are made at one end, and all deletions from the list are made at the other end.  The element which is first pushed into the order, the delete operation is first performed on that.\n\nFIFO Principle of Queue:\nA Queue is like a line waiting to purchase tickets, where the first person in line is the first person served. (i.e. First come first serve).\nPosition of the entry in a queue ready to be served, that is, the first entry that will be removed from the queue, is called the front of the queue(sometimes, head of the queue), similarly, the position of the last entry in the queue, that is, the one most recently added, is called the rear (or the tail) of the queue. See the below figure.\n\nCharacteristics of Queue:\nQueue can handle multiple data.\nWe can access both ends.\nThey are fast and flexible.\n\nQueue Representation:\n\n1. Array Representation of Queue:\nLike stacks, Queues can also be represented in an array: In this representation, the Queue is implemented using the array. Variables used in this case are\nQueue: the name of the array storing queue elements.\nFront: the index where the first element is stored in the array representing the queue.\nRear: the index where the last element is stored in an array representing the queue"],
        ["C++\n\n\n// Creating an empty queue \n// A structure to represent a queue\nclass Queue {\npublic:    \n\tint front, rear, size;    \n\tunsigned cap;    \n\tint* arr;\n}; \n// Function to create a queue of given capacity\n// It initializes size of queue as 0\nQueue* createQueue(unsigned cap)\n{    \n\tQueue* queue = new Queue();    \n\tqueue->cap = cap;    \n\tqueue->front = queue->size = 0;     \n\tqueue->rear = cap - 1;    \n\tqueue->arr = new int[(queue->cap * sizeof(int))];   \n\t return queue;\n}\n\nTypes of Queue:There are different types of queues:  \nInput Restricted Queue: This is a simple queue. In this type of queue, the input can be taken from only one end but deletion can be done from any of the ends. \nOutput Restricted Queue: This is also a simple queue. In this type of queue, the input can be taken from both ends but deletion can be done from only one end.\nCircular Queue: This is a special type of queue where the last position is connected back to the first position. Here also the operations are performed in FIFO order. To know more refer this.\n Double-Ended Queue (Dequeue): In a double-ended queue the insertion and deletion operations, both can be performed from both ends. To know more refer this.\nPriority Queue: A priority queue is a special queue where the elements are accessed based on the priority assigned to them."],
        ["Basic Operations for Queue in Data Structure: \nSome of the basic operations for Queue in Data Structure are: \n Enqueue() – Adds (or stores) an element to the end of the queue..\nDequeue() – Removal of elements from the queue.\nPeek() or front()- Acquires the data element available at the front node of the queue without deleting it. \nrear() – This operation returns the element at the rear end without removing it. \nisFull() – Validates if the queue is full. \nisNull() – Checks if the queue is empty. \nThere are a few supporting operations (auxiliary operations):\n 1. Enqueue():    \n Enqueue() operation in Queue adds (or stores) an element to the end of the queue. \n The following steps should be taken to enqueue (insert) data into a queue:   \n Step 1: Check if the queue is full. \n Step 2: If the queue is full, return overflow error and exit. \n Step 3: If the queue is not full, increment the rear pointer to point to the next empty space. \n Step 4: Add the data element to the queue location, where the rear is pointing. \n Step 5: return success.\nC++ \n void queueEnqueue(int data) \n {\n    \t // Check queue is full or not \n     \tif (capacity == rear) { \n       \t\t  printf('Queue is full'); \n         \t\treturn; \n    \t }  \n   \n     \t// Insert element at the rear  \n     \telse { \n       \t\t  queue[rear] = data; \n       \t\t rear++; \n     \t} \n     \treturn; \n}"],
        ["2. Dequeue(): \nRemoves (or access) the first element from the queue.\nThe following steps are taken to perform the dequeue operation:\n\nStep 1: Check if the queue is empty.\nStep 2: If the queue is empty, return the underflow error and exit.\nStep 3: If the queue is not empty, access the data where the front is pointing.\nStep 4: Increment the front pointer to point to the next available data element.\nStep 5: The Return success.\nvoid queueDequeue() \n {    \n  // If queue is empty     \n\t if (front == rear) {     \n   \t\t  printf('Queue is empty');     \n    \t\t return;      \n }\n // Shift all the elements from index 2     \n // till rear to the left by one     \n \telse {    \n    \t\t for (int i = 0; i < rear - 1; i++) {     \n       \t\t\t  queue[i] = queue[i + 1];      \n     \t}         \n    // decrement rear     \n    \t rear--;      \n}     \n return;"],["Time complexity: All the operations have O(1) time complexity. \nAuxiliary Space: O(N)   \nApplications of Queue: \nApplication of queue is common. In a computer system, there may be queues of tasks waiting for the printer, for access to disk storage, or even in a time-sharing system, for use of the CPU. Within a single program, there may be multiple requests to be kept in a queue, or one task may create other tasks, which must be done in turn by keeping them in a queue.  \nIt has a single resource and multiple consumers. \nIt synchronizes between slow and fast devices. \nIn a network, a queue is used in devices such as a router/switch and mail queue. \nVariations: dequeue, priority queue and double-ended priority queue."]]}],
        "linkedlist":[{"text":[["Linked List is a linear data structure, in which elements are not stored at a contiguous location, rather they are linked using pointers. Linked List forms a series of connected nodes, where each node stores the data and the address of the next node.\n\nNode Structure: A node in a linked list typically consists of two components:\nData: It holds the actual value or data associated with the node. \nNext Pointer: It stores the memory address (reference) of the next node in the sequence.\nHead and Tail: The linked list is accessed through the head node, which points to the first node in the list. The last node in the list points to NULL or nullptr, indicating the end of the list. This node is known as the tail node. \n\nWhy linked list data structure needed? \nHere are a few advantages of a linked list that is listed below, it will help you understand why it is necessary to know.\nDynamic Data structure: The size of memory can be allocated or de-allocated at run time based on the operation insertion or deletion. \n.Ease of Insertion/Deletion: The insertion and deletion of elements are simpler than arrays since no elements need to be shifted after insertion and deletion, Just the address needed to be updated. \n.Efficient Memory Utilization: As we know Linked List is a dynamic data structure the size increases or decreases as per the requirement so this avoids the wastage of memory.  \n.Implementation: Various advanced data structures can be implemented using a linked list like a stack, queue, graph, hash maps, etc.\n\nTypes of linked lists: \n There are mainly three types of linked lists: \n Single-linked list\n Double linked list\n Circular linked list\n 1. Single-linked list: \n In a singly linked list, each node contains a reference to the next node in the sequence. Traversing a singly linked list is done in a forward direction.\n\n2. Double-linked lis\n In a doubly linked list, each node contains references to both the next and previous nodes. This allows for traversal in both forward and backward directions, but it requires additional memory for the backward reference.\n\n3. Circular linked lis\n In a circular linked list, the last node points back to the head node, creating a circular structure. It can be either singly or doubly linked.\n"],
        ["Operations on Linked Lists:\n\n-Insertion: Adding a new node to a linked list involves adjusting the pointers of the existing nodes to maintain the proper sequence. Insertion can be performed at the beginning, end, or any position within the list\n-Deletion: Removing a node from a linked list requires adjusting the pointers of the neighboring nodes to bridge the gap left by the deleted node. Deletion can be performed at the beginning, end, or any position within the list.\n- Searching: Searching for a specific value in a linked list involves traversing the list from the head node until the value is found or the end of the list is reached.\n- Advantages of Linked Lists\n- Dynamic Size: Linked lists can grow or shrink dynamically, as memory allocation is done at runtime.\n- Insertion and Deletion: Adding or removing elements from a linked list is efficient, especially for large lists.\n- Flexibility: Linked lists can be easily reorganized and modified without requiring a contiguous block of memory.\n- Disadvantages of Linked Lists\n- Random Access: Unlike arrays, linked lists do not allow direct access to elements by index. Traversal is required to reach a specific node.\n- Extra Memory: Linked lists require additional memory for storing the pointers, compared to arrays.  \n\n  Conclusion:   \n Linked lists are versatile data structures that provide dynamic memory allocation and efficient insertion and deletion operations. Understanding the basics of linked lists is essential for any programmer or computer science enthusiast. With this knowledge, you can implement linked lists to solve various problems and expand your understanding of data structures and algorithms."]]}],
        "tree":[{"text":[["What is Binary Tree Data Structure?\n\n Binary Tree is defined as a tree data structure where each node has at most 2 children. Since each element in a binary tree can have only 2 children, we typically name them the left and right child.\n\nBinary Tree Representation\n A Binary tree is represented by a pointer to the topmost node (commonly known as the “root”) of the tree. If the tree is empty, then the value of the root is NULL. Each node of a Binary Tree contains the following parts:  \n  Data\n  Pointer to left child\n  Pointer to right child\n\nBasic Operation On Binary Tree:  \n  Inserting an element.\n  Removing an element.\n  Searching for an element.\n  Traversing the tree.\n\nAuxiliary Operation On Binary Tree:  \n  Finding the height of the tree\n  Find the level of a node of the tree\n Finding the size of the entire tree.\n\nThe topmost node in a binary tree is called the root, and the bottom-most nodes are called leaves. A binary tree can be visualized as a hierarchical structure with the root at the top and the leaves at the bottom.Binary trees have many applications in computer science, including data storage and retrieval, expression evaluation, network routing, and game AI. They can also be used to implement various algorithms such as searching, sorting, and graph algorithms."],
        ["In C, we can represent a tree node using structures. In other languages, we can use classes as part of their OOP feature. Below is an example of a tree node with integer data.\n\nC++:\n\nstruct node {  \n\t  int data;   \n\t struct node* left;    \n\tstruct node* right;\n}; \n// Method 2: Using 'class' to make\n// user-define data type\nclass Node {\npublic:  \n\t  int data;    \n\tNode* left;   \n\t Node* right;\n};\n\nBasic Operations On Binary Tree:\n Inserting an element.\n Removing an element.\n Searching for an element.\n Deletion for an element.\n Traversing an element. There are four (mainly three) types of traversals in a binary tree which will be discussed ahead.\n\n Auxiliary Operations On Binary Tree:\n Finding the height of the tree\n Find the level of the tree\n Finding the size of the entire tree."],
        ["Applications of Binary Tree:\n-In compilers, Expression Trees are used which is an application of binary trees.\n- Huffman coding trees are used in data compression algorithms.\n- Priority Queue is another application of binary tree that is used for searching maximum or minimum in O(1) time complexity.\n- Represent hierarchical data. \n- Used in editing software like Microsoft Excel and spreadsheets. \n- Useful for indexing segmented at the database is useful in storing cache in the system, \n- Syntax trees are used for most famous compilers for programming like GCC, and AOCL to perform arithmetic operations. \n- For implementing priority queues. \n- Used to find elements in less time (binary search tree) \n- Used to enable fast memory allocation in computers.   \n- Used to perform encoding and decoding operations. \n- Binary trees can be used to organize and retrieve information from large datasets, such as in inverted index and k-d trees. \n- Binary trees can be used to represent the decision-making process of computer-controlled characters in games, such as in decision trees. \n-  Binary trees can be used to implement searching algorithms, such as in binary search trees which can be used to quickly find an element in a sorted list.\n- Binary trees can be used to implement sorting algorithms, such as in heap sort which uses a binary heap to sort elements efficiently.\n\n Binary Tree Traversals:\n- Tree Traversal algorithms can be classified broadly into two categories:\n\n- Depth-First Search (DFS) Algorithms\n- Breadth-First Search (BFS) Algorithms"],
        ["Tree Traversal using Depth-First Search (DFS) algorithm can be further classified into three categories: \n\n Preorder Traversal (current-left-right): Visit the current node before visiting any nodes inside the left or right subtrees. Here, the traversal is root – left child – right child. It means that the root node is traversed first then its left child and finally the right child. \n\n Inorder Traversal (left-current-right): Visit the current node after visiting all nodes inside the left subtree but before visiting any node within the right subtree. Here, the traversal is left child – root – right child.  It means that the left child is traversed first then its root node and finally the right child.  \n\nPostorder Traversal (left-right-current): Visit the current node after visiting all the nodes of the left and right subtrees.  Here, the traversal is left child – right child – root.  It means that the left child has traversed first then the right child and finally its root node.  \n\nTree Traversal using Breadth-First Search (BFS) algorithm can be further classified into one category:   \n\n Level Order Traversal:  Visit nodes level-by-level and left-to-right fashion at the same level. Here, the traversal is level-wise. It means that the most left child has traversed first and then the other children of the same level from left to right have traversed.\n\nSome extra properties of binary tree are:\n-Each node in a binary tree can have at most two child nodes: In a binary tree, each node can have either zero, one, or two child nodes. If a node has zero children, it is called a leaf node. If a node has one child, it is called a unary node. If a node has two children, it is called a binary node.\n- The node at the top of the tree is called the root node: The root node is the first node in a binary tree and all other nodes are connected to it. All other nodes in the tree are either child nodes or descendant nodes of the root node.\n-Nodes that do not have any child nodes are called leaf nodes: Leaf nodes are the endpoints of the tree and have no children. They represent the final result of the tree.\n-The height of a binary tree is defined as the number of edges from the root node to the deepest leaf node: The height of a binary tree is the length of the longest path from the root node to any of the leaf nodes. The height of a binary tree is also known as its depth.\n-In a full binary tree, every node except the leaves has exactly two children: In a full binary tree, all non-leaf nodes have exactly two children. This means that there are no unary nodes in a full binary tree.\n-In a complete binary tree, every level of the tree is completely filled except for the last level, which can be partially filled: In a complete binary tree, all levels of the tree except the last level are completely filled. This means that there are no gaps in the tree and all nodes are connected to their parent nodes.\n-In a balanced binary tree, the height of the left and right subtrees of every node differ by at most 1: In a balanced binary tree, the height of the left and right subtrees of every node is similar. This ensures that the tree is balanced and that the height of the tree is minimized.\n-The in-order, pre-order, and post-order traversal of a binary tree are three common ways to traverse the tree: In-order, pre-order, and post-order are three different ways to traverse a binary tree. In-order traversal visits the left subtree, the node itself, and then the right subtree. Pre-order traversal visits the node itself, the left subtree, and then the right subtree. Post-order traversal visits the left subtree, the right subtree, and then the node itself."]]}],
        "graph":[{"text":[["What is Graph Data Structure?\n\n A Graph is a non-linear data structure consisting of vertices and edges. The vertices are sometimes also referred to as nodes and the edges are lines or arcs that connect any two nodes in the graph. More formally a Graph is composed of a set of vertices( V ) and a set of edges( E ). The graph is denoted by G(E, V).\n\nComponents of a Graph\nVertices: Vertices are the fundamental units of the graph. Sometimes, vertices are also known as vertex or nodes. Every node/vertex can be labeled or unlabelled.\nEdges: Edges are drawn or used to connect two nodes of the graph. It can be ordered pair of nodes in a directed graph. Edges can connect any two nodes in any possible way. There are no rules. Sometimes, edges are also known as arcs. Every edge can be labeled/unlabelled.\nGraphs are used to solve many real-life problems. Graphs are used to represent networks. The networks may include paths in a city or telephone network or circuit network. Graphs are also used in social networks like linkedIn, Facebook. For example, in Facebook, each person is represented with a vertex(or node). Each node is a structure and contains information like person id, name, gender, locale etc.\n\nGraph data structures are a powerful tool for representing and analyzing complex relationships between objects or entities. They are particularly useful in fields such as social network analysis, recommendation systems, and computer networks. In the field of sports data science, graph data structures can be used to analyze and understand the dynamics of team performance and player interactions on the fields into team performance and player dynamics in sports.\n\nTypes Of Graph\n1. Null Graph\n A graph is known as a null graph if there are no edges in the graph.   \n  2. Trivial Graph\n  Graph having only a single vertex, it is also the smallest graph possible.\n  3. Undirected Graph\nA graph in which edges do not have any direction. That is the nodes are unordered pairs in the definition of every edge. \n4. Directed Graph\nA graph in which edge has direction. That is the nodes are ordered pairs in the definition of every edge.\n5. Connected Graph\nThe graph in which from one node we can visit any other node in the graph is known as a connected graph. \n6. Disconnected Graph\nThe graph in which at least one node is not reachable from a node is known as a disconnected graph.\n7. Regular Graph\nThe graph in which the degree of every vertex is equal to K is called K regular graph.\n8. Complete Graph\nThe graph in which from each node there is an edge to each other node."],
        ["9.Cycle Graph\nThe graph in which the graph is a cycle in itself, the degree of each vertex is 2. \n\n10. Cyclic Graph\nA graph containing at least one cycle is known as a Cyclic graph.\n\n"]]}],
        "hash":[{"text":[["data not available yet"]]}],
        "trie":[{"text":[["data not available yet"]]}],
        "segtree":[{"text":[["data not available yet"]]}],
        "unordered_map":[{"text":[["data not available yet"]]}],
        "set":[{"text":[["data not available yet"]]}]


      },
    {
        "space":[{"text":[["data not available yet"]]}],
        "time":[{"text":[["data not available yet"]]}],
        "sort":[{"text":[["data not available yet"]]}],
        "search":[{"text":[["data not available yet"]]}],
        "recursion":[{"text":[["data not available yet"]]}],
        "greedy":[{"text":[["data not available yet"]]}]
    },
    {
        "Basics":[{"text":[["An operating system acts as an intermediary between the user of a computer and computer hardware.The purpose of an operating system is to provide an environment in which a user can execute programs conveniently and efficiently.An operating system is software that manages computer hardware. The hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system.\nA more common definition is that the operating system is the one program running at all times on the computer (usually called the kernel), with all else being application programs.An operating system is concerned with the allocation of resources and services, such as memory, processors, devices, and information.\nThe operating system correspondingly includes programs to manage these resources, such as a traffic controller, a scheduler, a memory management module, I/O programs, and a file system.\n\n\nCharacteristics of Operating Systems\nLet us now discuss some of the important characteristic features of operating systems:e\nDevice Management: The operating system keeps track of all the devices. So, it is also called the Input/Output controller that decides which process gets the device, when, and for how much tie\nFile Management: It allocates and de-allocates the resources and also decides who gets the resoure\nJob Accounting: It keeps track of time and resources used by various jobs or usee\nError-detecting Aids: These contain methods that include the production of dumps, traces, error messages, and other debugging and error-detecting methoe\nMemory Management: It keeps track of the primary memory, like what part of it is in use by whom, or what part is not in use, etc. and It also allocates the memory when a process or program requests e\nProcessor Management: It allocates the processor to a process and then de-allocates the processor when it is no longer required or the job is doe\nControl on System Performance: It records the delays between the request for a service and the syste\nSecurity: It prevents unauthorized access to programs and data using passwords or some kind of protection techniqe\nConvenience: An OS makes a computer more convenient to ue\nEfficiency: An OS allows the computer system resources to be used efficiente\nAbility to Evolve: An OS should be constructed in such a way as to permit the effective development, testing, and introduction of new system functions at the same time without interfering with servie\nThroughput: An OS should be constructed so that It can give maximum throughput (Number of tasks per unit time\n"]
        ,["STORAGE DEFINITIONS AND NOTATION\n\nThe basic unit of computer storage is the bit. A bit can contain one of two values, 0 and 1. All other storage in a computer is based on collections of bits. Given enough bits, it is amazing how many things a computer can represent: numbers, letters, images, movies, sounds, documents, and programs, to name a few. A byte is 8 bits, and on most computers it is the smallest convenient chunk of storage. For example, most computers don’t have an instruction to move a bit but do have one to move a byte. A less common term is word, which is a given computer architecture’s native unit of data. A word is made up of one or more bytes. For example, a computer that has 64-bit registers and 64-bit memory addressing typically has 64-bit (8-byte) words. A computer executes many operations in its native word size rather than a byte at a time.\nComputer storage, along with most computer throughput, is generally measured and manipulated in bytes and collections of bytes. A kilobyte, or KB, is 1,024 bytes; a megabyte, or MB, is 1,0242 bytes; a gigabyte, or GB, is 1,0243 bytes; a terabyte, or TB, is 1,0244 bytes; and a petabyte, or PB, is 1,0245 bytes. Computer manufacturers often round off these numbers and say that a megabyte is 1 million bytes and a gigabyte is 1 billion bytes. Networking measurements are an exception to this general rule; they are given in bits (because networks move data a bit at a time).\n\n\nFunctionalities of Operating System\n\n.Resource Management: When parallel accessing happens in the OS means when multiple users are accessing the system the OS works as Resource Manager, Its responsibility is to provide hardware to the user. It decreases the load in the system.\n\n.Process Management: It includes various tasks like scheduling and termination of the process. It is done with the help of CPU Scheduling algorithms.\n\n.Storage Management: The file system mechanism used for the management of the storage. NIFS, CIFS, CFS, NFS, etc. are some file systems. All the data is stored in various tracks of Hard disks that are all managed by the storage manager. It included Hard Disk.\n.Memory Management: Refers to the management of primary memory. The operating system has to keep track of how much memory has been used and by whom. It has to decide which process needs memory space and how much. OS also has to allocate and deallocate the memory space.\n\n.Security/Privacy Management: Privacy is also provided by the Operating system using passwords so that unauthorized applications can’t access programs or data. For example, Windows uses Kerberos authentication to prevent unauthorized access to data."]
        ,["The process operating system as User Interface:\n\n\n1.User\n\n2.System and application program\n\n3.Operating system\n\n4.Hardware\n\nEvery general-purpose computer consists of hardware, an operating system(s), system programs, and application programs. The hardware consists of memory, CPU, ALU, I/O devices, peripheral devices, and storage devices. The system program consists of compilers, loaders, editors, OS, etc. The application program consists of business programs and database programs.\n\nEvery computer must have an operating system to run other programs. The operating system coordinates the use of the hardware among the various system programs and application programs for various users. It simply provides an environment within which other programs can do useful work. \nThe operating system is a set of special programs that run on a computer system that allows it to work properly. It performs basic tasks such as recognizing input from the keyboard, keeping track of files and directories on the disk, sending output to the display screen, and controlling peripheral devices."],
        ["Purposes and Tasks of Operating Systems\n\nSeveral tasks are performed by the Operating Systems and it also helps in serving a lot of purposes which are mentioned below. We will see how Operating System helps us in serving in a better way with the help of the task performed by it.\n\nTasks of an Operating System\n\n1.Provides the facilities to create and modify programs and data files using an editor.\n2.Access to the compiler for translating the user program from high-level language to machine language.\n3.Provide a loader program to move the compiled program code to the computer’s memory for execution.\n4.Provide routines that handle the details of I/O programming.I/O System Management\n\nThe module that keeps track of the status of devices is called the I/O traffic controller. Each I/O device has a device handler that resides in a separate process associated with that device. \nThe I/O subsystem consists of  \n1.A memory Management component that includes buffering caching and spooling.\n2.A general device driver interface.\n\nDrivers for Specific Hardware DevicesBelow mentioned are the drivers which are required for a specific Hardware Device. Here we discussed Assemblers, compilers, and interpreters, loaders.\n\nAssembler\nThe input to an assembler is an assembly language program. The output is an object program plus information that enables the loader to prepare the object program for execution. At one time, the computer programmer had at his disposal a basic machine that interpreted, through hardware, certain fundamental instructions. He would program this computer by writing a series of ones and Zeros (Machine language) and placing them into the memory of the machine. Examples of assembly languages include "],
        ["Compiler and Interpreter\n\nThe High-level languages– examples are C, C++, Java, Python, etc (around 300+ famous high-level languages) are processed by compilers and interpreters. A compiler is a program that accepts a source program in a “high-level language “and produces machine code in one go. Some of the compiled languages are FORTRAN, COBOL, C, C++, Rust, and Go. An interpreter is a program that does the same thing but converts high-level code to machine code line-by-line and not all at once. Examples of interpreted languages are \n1.Python \n2.Perl\n3.Ruby\n\nLoader\nA Loader is a routine that loads an object program and prepares it for execution. There are various loading schemes: absolute, relocating, and direct-linking. In general, the loader must load, relocate and link the object program. The loader is a program that places programs into memory and prepares them for execution. In a simple loading scheme, the assembler outputs the machine language translation of a program on a secondary device and a loader places it in the core. The loader places into memory the machine language version of the user’s program and transfers control to it. Since the loader program is much smaller than the assembler, those make more core available to the user’s program.\nComponents of an Operating Systems\nThere are two basic components of an Operating System.\nShell\nKernel\nShell is the outermost layer of the Operating System and it handles the interaction with the user. The main task of the Shell is the management of interaction between the User and OS. Shell provides better communication with the user and the Operating System Shell does it by giving proper input to the user it also interprets input for the OS and handles the output from the OS. It works as a way of communication between the User and the OS.\nThe kernel is one of the components of the Operating System which works as a core component. The rest of the components depends on Kernel for the supply of the important services that are provided by the Operating System. The kernel is the primary interface between the Operating system and Hardware. \nFunctions of Kernel\nThe following functions are to be performed by the Kernel.\nIt helps in controlling the System Calls.\nIt helps in I/O Management.\nIt helps in the management of applications, memory, etc."]]}],
        "Kernel":[{"text":[["Kernel is the core part of an operating system that manages system resources. It also acts as a bridge between the application and hardware of the computer. It is one of the first programs loaded on start-up (after the Bootloader).\n\nKernel mode and User mode of CPU operation The CPU can execute certain instructions only when it is in kernel mode. These instructions are called privilege instruction. They allow the implementation of special operations whose execution by the user program could interface with the functioning of the operating system or activity of another user program. For example, instruction for managing memory protection. \n.The operating system puts the CPU in kernel mode when it is executing in the kernel so, that kernel can execute some special operation.\n.The operating system puts the CPU in user mode when a user program is in execution so, that the user program cannot interface with the operating system program.\n.User-level instruction does not require special privilege. Example are ADD,PUSH,etc.\n\nThe concept of modes can be extended beyond two, requiring more than a single mode bit CPUs that support virtualization. It uses one of these extra bits to indicate when the virtual machine manager, VMM, is in control of the system. The VMM has more privileges than ordinary user programs, but not so many as the full kernel. \nSystem calls are typically implemented in the form of software interrupts, which causes the hardware’s interrupt handler to transfer control over to an appropriate interrupt handler, which is part of the operating system, switching the bit mode to kernel mode in the process. The interrupt handler checks exactly which interrupt was generated, checks additional parameters ( generally passed through registers ) if appropriate, and then calls the appropriate kernel service routine to handle the service requested by the system call. \nUser programs’ attempts to execute illegal instructions ( privileged or non-existent instructions ), or to access forbidden memory areas, also generate software interrupts, which are trapped by the interrupt handler, and control is transferred to the OS, which issues an appropriate error message, possibly dumps data to a log ( core ) file for later analysis, and then terminates the offending program.\n\nWhat is Microkernel? \n\nA microkernel is one of the classifications of the kernel. Being a kernel it manages all system resources. But in a microkernel, the user services and kernel services are implemented in different address spaces. The user services are kept in user address space, and kernel services are kept under kernel address space, thus also reduces the size of kernel and size of an operating system as well.\n\nIt provides minimal services of process and memory management. The communication between client program/application and services running in user address space is established through message passing, reducing the speed of execution microkernel. The Operating System remains unaffected as user services and kernel services are isolated so if any user service fails it does not affect kernel service. Thus it adds to one of the advantages of a microkernel. It is easily extendible i.e. if any new services are to be added they are added to user address space and hence require no modification in kernel space. It is also portable, secure, and reliable. Examples of microkernel-based operating systems include L4, QNX, and MINIX"],
        ["Microkernel Architecture – \n\nSince the kernel is the core part of the operating system, so it is meant for handling the most important services only. Thus in this architecture, only the most important services are inside the kernel and the rest of the OS services are present inside the system application program. Thus users are able to interact with those not-so-important services within the system application. And the microkernel is solely responsible for the most important services of the operating system they are named as follows: \n1.Inter process-Communication\n2.Memory Management\n3.CPU-Scheduling\n\nSome key features of microkernel-based operating systems include:\nSmall and simple kernel:\n The microkernel is designed to be as small and simple as possible, containing only the basic functions needed to manage system resources, such as memory, processes, and interprocess communication.\n1.Modular design: Most of the operating system’s services and drivers are moved into user space, where they can be loaded and unloaded as needed. This allows for a more modular and flexible design, and makes it easier to add or remove functionality from the system.\n2.Message passing: Communication between different parts of the operating system is typically done using message passing, rather than shared memory. This provides a more secure and reliable way to exchange information between processes, and helps prevent bugs and errors from propagating throughout the system.\n3.Extensibility: The microkernel design makes it easier to add new functionality to the operating system, since new services and drivers can be added to user space without modifying the kernel itself.\n4.Security: By separating the kernel from most of the operating system’s services, microkernel-based operating systems can be more secure, since bugs and vulnerabilities in user-space code are less likely to affect the kernel. Additionally, microkernel-based systems often use formal verification techniques to ensure the correctness of the kernel’s code.\nHow does a microkernel architecture make an operating system more modular and flexible?\nServices are implemented as user-level processes, which makes it easier to add, remove, or replace services without affecting other parts of the system. This makes it easier to customize the operating system to meet specific requirements."],["\n\nAdvantages of Microkernel – \n1.Modularity: Because the kernel and servers can be developed and maintained independently, the microkernel design allows for greater modularity. This can make adding and removing features and services from the system easier.\n2.Fault isolation: The microkernel design aids in the isolation of faults and their prevention from affecting the entire system. If a server or other component fails, it can be restarted or replaced without causing any disruptions to the rest of the system.\n3.Performance: Because the kernel only contains the essential functions required to manage the system, the microkernel design can improve performance. This can make the system faster and more efficient.\n4.Security: The microkernel design can improve security by reducing the system’s attack surface by limiting the functions provided by the kernel. Malicious software may find it more difficult to compromise the system as a result of this.\n5.Reliability: Microkernels are less complex than monolithic kernels, which can make them more reliable and less prone to crashes or other issues.\n6.Scalability: Microkernels can be easily scaled to support different hardware architectures, making them more versatile\n7.Portability: Microkernels can be ported to different platforms with minimal effort, which makes them useful for embedded systems and other specialized applications.\n8.Eclipse IDE is a good example of Microkernel Architecture. "]]}],
        "scheduling":[{"text":[["Scheduling of processes/work is done to finish the work on time. CPU Scheduling is a process that allows one process to use the CPU while another process is delayed (in standby) due to unavailability of any resources such as I / O etc, thus making full use of the CPU. The purpose of CPU Scheduling is to make the system more efficient, faster, and fairer.\nWhenever the CPU becomes idle, the operating system must select one of the processes in the line ready for launch. The selection process is done by a temporary (CPU) scheduler. The Scheduler selects between memory processes ready to launch and assigns the CPU to one of them.\n\n1. First Come First Serve: 1.FCFS considered to be the simplest of all operating system scheduling algorithms. First come first serve scheduling algorithm states that the process that requests the CPU first is allocated the CPU first and is implemented by using FIFO queue.\nCharacteristics of FCFS:\n1.FCFS supports non-preemptive and preemptive CPU scheduling algorithms.\n2.Tasks are always executed on a First-come, First-serve concept.\n3.FCFS is easy to implement and use.\n4.This algorithm is not much efficient in performance, and the wait time is quite high.\n\nAdvantages of FCFS:\nEasy to implement\nFirst come, first serve method\n\nDisadvantages of FCFS:\nFCFS suffers from Convoy effect.\nThe average waiting time is much higher than the other algorithms.\nFCFS is very simple and easy to implement and hence not much efficient."],
        ["2. Shortest Job First(SJF):\n\nShortest job first (SJF) is a scheduling process that selects the waiting process with the smallest execution time to execute next. This scheduling method may or may not be preemptive. Significantly reduces the average waiting time for other processes waiting to be executed. The full form of SJF is Shortest Job First.\n\n\nCharacteristics of SJF:.Shortest Job first has the advantage of having a minimum average waiting time among all operating system scheduling algorithms.\n.It is associated with each task as a unit of time to complete.\n.It may cause starvation if shorter processes keep coming. This problem can be solved using the concept of ageing.\n\nAdvantages of Shortest Job first:\n.As SJF reduces the average waiting time thus, it is better than the first come first serve scheduling algorithm.\n.SJF is generally used for long term scheduling\n\nDisadvantages of SJF: \n.One of the demerit SJF has is starvation.\n.Many times it becomes complicated to predict the length of the upcoming CPU request"],
        ["3. Longest Job First(LJF):\n\nLongest Job First(LJF) scheduling process is just opposite of shortest job first (SJF), as the name suggests this algorithm is based upon the fact that the process with the largest burst time is processed first. Longest Job First is non-preemptive in nature\nCharacteristics of LJF:\nAmong all the processes waiting in a waiting queue, CPU is always assigned to the process having largest burst time.\nIf two processes have the same burst time then the tie is broken using FCFS i.e. the process that arrived first is processed first.\n\nLJF CPU Scheduling can be of both preemptive and non-preemptive types.\n\nAdvantages of LJF:\n.No other task can schedule until the longest job or process executes completely\n.All the jobs or processes finish at the same time approximately.\n.Disadvantages of LJF:\n\nGenerally, the LJF algorithm gives a very high average waiting time and average turn-around time for a given set of processes.\n\nThis may lead to convoy effect."],
        ["4. Priority Scheduling:\n\n\n Preemptive Priority CPU Scheduling Algorithm is a pre-emptive method of CPU scheduling algorithm that works based on the priority of a process. In this algorithm, the editor sets the functions to be as important, meaning that the most important process must be done first. In the case of any conflict, that is, where there are more than one processor with equal value, then the most important CPU planning algorithm works on the basis of the FCFS (First Come First Serve) algorithm.\n\n Characteristics of Priority Scheduling:\n.Schedules tasks based on priority.\n.When the higher priority work arrives while a task with less priority is executed, the higher priority work takes the place of the less priority one and\n.The latter is suspended until the execution is complete.\n.Lower is the number assigned, higher is the priority level of a process.\n\n\nAdvantages of Priority Scheduling:\n.The average waiting time is less than FCFS\n.Less complex\n\n\nDisadvantages of Priority Scheduling:\n.One of the most common demerits of the Preemptive priority CPU scheduling algorithm is the Starvation Problem. This is the problem in which a process has to wait for a longer amount of time to get scheduled into the CPU. This condition is called the starvation problem."],
        ["5. Round robin:\n\n Round Robin is a CPU scheduling algorithm where each process is cyclically assigned a fixed time slot. It is the preemptive version of First come First Serve CPU Scheduling algorithm. Round Robin CPU Algorithm generally focuses on Time Sharing technique. \n Characteristics of Round robin:\n It’s simple, easy to use, and starvation-free as all processes get the balanced CPU allocation\nOne of the most widely used methods in CPU scheduling as a core\nIt is considered preemptive as the processes are given to the CPU for a very limited time\nAdvantages of Round robin:\nRound robin seems to be fair as every process gets an equal share of CPU\nThe newly created process is added to the end of the ready queue\nTo learn about how to implement this CPU scheduling algorithm, please refer to our detailed article on the Round robin Scheduling algorithm.\n\n\n6. Shortest Remaining Time First:\nShortest remaining time first is the preemptive version of the Shortest job first which we have discussed earlier where the processor is allocated to the job closest to completion. In SRTF the process with the smallest amount of time remaining until completion is selected to execute.\nCharacteristics of Shortest remaining time first:\nSRTF algorithm makes the processing of the jobs faster than SJF algorithm, given it’s overhead charges are not counted.\nThe context switch is done a lot more times in SRTF than in SJF and consumes the CPU’s valuable time for processing. This adds up to its processing time and diminishes its advantage of fast processing\nAdvantages of SRTF:\nIn SRTF the short processes are handled very fast\nThe system also requires very little overhead since it only makes a decision when a process completes or a new process is added.\nDisadvantages of SRTF:\nLike the shortest job first, it also has the potential for process starvation.\nLong processes may be held off indefinitely if short processes are continually added. "]]}],
        "process":[{"text":[["A process is a program in execution. For example, when we write a program in C or C++ and compile it, the compiler creates binary code. The original code and binary code are both programs. When we actually run the binary code, it becomes a process.\n\nA process is an ‘active’ entity instead of a program, which is considered a ‘passive’ entity. A single program can create many processes when run multiple times; for example, when we open a .exe or binary file multiple times, multiple instances begin (multiple processes are created). \n\nProcess management refers to the techniques and strategies used by organizations to design, monitor, and control their business processes to achieve their goals efficiently and effectively. It involves identifying the steps involved in completing a task, assessing the resources required for each step, and determining the best way to execute the task.\nProcess management can help organizations improve their operational efficiency, reduce costs, increase customer satisfaction, and maintain compliance with regulatory requirements. It involves analyzing the performance of existing processes, identifying bottlenecks, and making changes to optimize the process flow.\n\nProcess management includes various tools and techniques such as process mapping, process analysis, process improvement, process automation, and process control. By applying these tools and techniques, organizations can streamline their processes, eliminate waste, and improve productivity.Overall, process management is a critical aspect of modern business operations and can help organizations achieve their goals and stay competitive in today’s rapidly changing marketplace.\nProcess Management\nIf the operating system supports multiple users then services under this are very important. In this regard, operating systems have to keep track of all the completed processes, Schedule them, and dispatch them one after another. But the user should feel that he has full control of the CPU.\nSome of the systems call in this category are as follows.\n\nCreate a child’s process identical to the parent’s. \nTerminate a process\nWait for a child process to terminate\nChange the priority of the process\nBlock the process\nReady the process \nDispatch a process\nSuspend a process\nResume a process\nDelay a process\nFork a process"],
        ["Explanation of Process \n\nText Section: A Process, sometimes known as the Text Section, also includes the current activity represented by the value of the Program Counter. \nStack: The stack contains temporary data, such as function parameters, returns addresses, and local variables. \nData Section: Contains the global variable. \nHeap Section: Dynamically allocated memory to process during its run time.\n\nAttributes or Characteristics of a Process\n A process has the following attributes.\n Process Id:    A unique identifier assigned by the operating system\nProcess State: Can be ready, running, etc.\nCPU registers: Like the Program Counter (CPU registers must be saved and restored when a process is swapped in and out of the CPU)\nAccounts information: Amount of CPU used for process execution, time limits, execution ID, etc\nI/O status information: For example, devices allocated to the process, open files, etc\nCPU scheduling information: For example, Priority (Different processes may have different priorities, for example, a shorter process assigned high priority in the shortest job first scheduling)\nAll of the above attributes of a process are also known as the context of the process. Every process has its own process control block(PCB), i.e. each process will have a unique PCB. All of the above attributes are part of the PCB.\n\nA process is in one of the following states: \nNew: Newly Created Process (or) being-created process.\nReady: After the creation process moves to the Ready state, i.e. the process is ready for execution.\nRun: Currently running process in CPU (only one process at a time can be under execution in a single processor)\nWait (or Block): When a process requests I/O access.\nComplete (or Terminated): The process completed its execution.\nSuspended Ready: When the ready queue becomes full, some processes are moved to a suspended ready state\nSuspended Block: When the waiting queue becomes full.\n\nContext Switching: The process of saving the context of one process and loading the context of another process is known as Context Switching. In simple terms, it is like loading and unloading the process from the running state to the ready state. "]]}],
        "deadlock":[{"text":[["A process in operating system uses resources in the following way. \n\nRequests a resource \nUse the resource \nReleases the resource \n\nA deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process. \nConsider an example when two trains are coming toward each other on the same track and there is only one track, none of the trains can move once they are in front of each other. A similar situation occurs in operating systems when there are two or more processes that hold some resources and wait for resources held by other(s). For example, in the below diagram, Process 1 is holding Resource 1 and waiting for resource 2 which is acquired by process 2, and process 2 is waiting for resource 1.\n\n Examples Of Deadlock\nThe system has 2 tape drives. P1 and P2 each hold one tape drive and each needs another one.\nSemaphores A and B, initialized to 1, P0, and P1 are in deadlock as follows:\nP0 executes wait(A) and preempts.\nP1 executes wait(B).\nNow P0 and P1 enter in deadlock.\n\n\nDeadlock can arise if the following four conditions hold simultaneously (Necessary Conditions) Mutual Exclusion: Two or more resources are non-shareable (Only one process can use at a time) \n\nHold and Wait: A process is holding at least one resource and waiting for resources. \n\nNo Preemption: A resource cannot be taken from a process unless the process releases the resource. \n\nCircular Wait: A set of processes waiting for each other in circular form."],
        ["Methods for handling deadlock \n\nThere are three ways to handle deadlock \n\n1) Deadlock prevention or avoidance:\nPrevention:\nThe idea is to not let the system into a deadlock state. This system will make sure that above mentioned four conditions will not arise. These techniques are very costly so we use this in cases where our priority is making a system deadlock-free.\nOne can zoom into each category individually, Prevention is done by negating one of the above-mentioned necessary conditions for deadlock. Prevention can be done in four different ways:\n1. Eliminate mutual exclusion\n2. Solve hold and Wait \n3. Allow preemption\n4. Circular wait Solution \n\nAvoidance:\nAvoidance is kind of futuristic. By using the strategy of “Avoidance”, we have to make an assumption. We need to ensure that all information about resources that the process will need is known to us before the execution of the process. We use Banker’s algorithm (Which is in turn a gift from Dijkstra) to avoid deadlock. \nIn prevention and avoidance, we get the correctness of data but performance decreases.\n2) Deadlock detection and recovery: If Deadlock prevention or avoidance is not applied to the software then we can handle this by deadlock detection and recovery. which consist of two phases:\nIn the first phase, we examine the state of the process and check whether there is a deadlock or not in the system.\nIf found deadlock in the first phase then we apply the algorithm for recovery of the deadlock.\n\nRecovery from Deadlock\n1. Manual Intervention:\nWhen a deadlock is detected, one option is to inform the operator and let them handle the situation manually. While this approach allows for human judgment and decision-making, it can be time-consuming and may not be feasible in large-scale systems.\n\n2. Automatic Recovery.\n An alternative approach is to enable the system to recover from deadlock automatically. This method involves breaking the deadlock cycle by either aborting processes or preempting resources. Let’s delve into these strategies in more detail."],
        ["Recovery from Deadlock: Process Termination\n1.Abort all deadlocked processes:\nThis approach breaks the deadlock cycle, but it comes at a significant cost. The processes that were aborted may have executed for a considerable amount of time, resulting in the loss of partial computations. These computations may need to be recomputed later.\n2. Abort one process at a time:\nInstead of aborting all deadlocked processes simultaneously, this strategy involves selectively aborting one process at a time until the deadlock cycle is eliminated. However, this incurs overhead as a deadlock-detection algorithm must be invoked after each process termination to determine if any processes are still deadlocked.\nFactors for choosing the termination order:\n– The process’s priority\n– Completion time and the progress made so far\n– Resources consumed by the process\n– Resources required to complete the process\n– Number of processes to be terminated\n– Process type (interactive or batch)\nRecovery from Deadlock: Resource Preemption:\n1. Selecting a victim:\nResource preemption involves choosing which resources and processes should be preempted to break the deadlock. The selection order aims to minimize the overall cost of recovery. Factors considered for victim selection may include the number of resources held by a deadlocked process and the amount of time the process has consumed.\n2. Rollback:\nIf a resource is preempted from a process, the process cannot continue its normal execution as it lacks the required resource. Rolling back the process to a safe state and restarting it is a common approach. Determining a safe state can be challenging, leading to the use of total rollback, where the process is aborted and restarted from scratch.\n3. Starvation prevention:\nTo prevent resource starvation, it is essential to ensure that the same process is not always chosen as a victim. If victim selection is solely based on cost factors, one process might repeatedly lose its resources and never complete its designated task. To address this, it is advisable to limit the number of times a process can be chosen as a victim, including the number of rollbacks in the cost factor.\n3) Deadlock ignorance: If a deadlock is very rare, then let it happen and reboot the system. This is the approach that both Windows and UNIX take. we use the ostrich algorithm for deadlock ignorance.\nIn Deadlock, ignorance performance is better than the above two methods but the correctness of data.\nSafe State:\nA safe state can be defined as a state in which there is no deadlock. It is achievable if:\nIf a process needs an unavailable resource, it may wait until the same has been released by a process to which it has already been allocated. if such a sequence does not exist, it is an unsafe state"]]}],
        "threads":[{"text":[["Within a program, a Thread is a separate execution path. It is a lightweight process that the operating system can schedule and run concurrently with other threads. The operating system creates and manages threads, and they share the same memory and resources as the program that created them. This enables multiple threads to collaborate and work efficiently within a single program.\n\nA thread is a single sequence stream within a process. Threads are also called lightweight processes as they possess some of the properties of processes. Each thread belongs to exactly one process. In an operating system that supports multithreading, the process can consist of many threads. But threads can be effective only if CPU is more than 1 otherwise two threads have to context switch for that single CPU.\n\nWhy Do We Need Thread?\n\nThreads run in parallel improving the application performance. Each such thread has its own CPU state and stack, but they share the address space of the process and the environment. \nThreads can share common data so they do not need to use interprocess communication. Like the processes, threads also have states like ready, executing, blocked, etc. \nPriority can be assigned to the threads just like the process, and the highest priority thread is scheduled first.\nEach thread has its own Thread Control Block (TCB). Like the process, a context switch occurs for the thread, and register contents are saved in (TCB). As threads share the same address space and resources, synchronization is also required for the various activities of the thread.\n\n\nWhy Multi-Threading? \nA thread is also known as a lightweight process. The idea is to achieve parallelism by dividing a process into multiple threads. For example, in a browser, multiple tabs can be different threads. MS Word uses multiple threads: one thread to format the text, another thread to process inputs, etc. More advantages of multithreading are discussed below.\n\nMultithreading is a technique used in operating systems to improve the performance and responsiveness of computer systems. Multithreading allows multiple threads (i.e., lightweight processes) to share the same resources of a single process, such as the CPU, memory, and I/O devices."],
        ["Difference Between Process and Thread\n\n\nThe primary difference is that threads within the same process run in a shared memory space, while processes run in separate memory spaces. Threads are not independent of one another like processes are, and as a result, threads share with other threads their code section, data section, and OS resources (like open files and signals). But, like a process, a thread has its own program counter (PC), register set, and stack space.          \n\nAdvantages of Thread\n\nResponsiveness: If the process is divided into multiple threads, if one thread completes its execution, then its output can be immediately returned.\n\nFaster context switch: Context switch time between threads is lower compared to the process context switch. Process context switching requires more overhead from the CPU. \n\nEffective utilization of multiprocessor system: If we have multiple threads in a single process, then we can schedule multiple threads on multiple processors. This will make process execution faster. \n\nResource sharing: Resources like code, data, and files can be shared among all threads within a process. Note: Stacks and registers can’t be shared among the threads. Each thread has its own stack and registers. \n\nCommunication: Communication between multiple threads is easier, as the threads share a common address space. while in the process we have to follow some specific communication techniques for communication between the two processes. \n\nEnhanced throughput of the system: If a process is divided into multiple threads, and each thread function is considered as one job, then the number of jobs completed per unit of time is increased, thus increasing the throughput of the system. "],
        ["Types of Threads\n\nThreads are of two types. These are described below.\nUser Level Thread \nKernel Level Thread\n\nUser Level Threads\nUser Level Thread is a type of thread that is not created using system calls. The kernel has no work in the management of user-level threads. User-level threads can be easily implemented by the user. In case when user-level threads are single-handed processes, kernel-level thread manages them. Let’s look at the advantages and disadvantages of User-Level Thread.\n\nAdvantages of User-Level Threads\nImplementation of the User-Level Thread is easier than Kernel Level Thread.\nContext Switch Time is less in User Level Thread.\nUser-Level Thread is more efficient than Kernel-Level Thread.\nBecause of the presence of only Program Counter, Register Set, and Stack Space, it has a simple representation.\n\nDisadvantages of User-Level Threads\nThere is a lack of coordination between Thread and Kernel.\nIn case of a page fault, the whole process can be blocked.\nKernel Level Threads\nA kernel Level Thread is a type of thread that can recognize the Operating system easily. Kernel Level Threads has its own thread table where it keeps track of the system. The operating System Kernel helps in managing threads. Kernel Threads have somehow longer context switching time. Kernel helps in the management of threads.\n\nAdvantages of Kernel-Level Threads\nIt has up-to-date information on all threads.\nApplications that block frequency are to be handled by the Kernel-Level Threads.\nWhenever any process requires more time to process, Kernel-Level Thread provides more time to it.\n\nDisadvantages of Kernel-Level threads\nKernel-Level Thread is slower than User-Level Thread.\nImplementation of this type of thread is a little more complex than a user-level thread."],
        ["Threads are not independent of each other as they share the code, data, OS resources etc.\n\nSimilarity between Threads and Processes \n1.Only one thread or process is active at a time \n2.Within process both execute sequential\n3.Both can create children \n4.Both can be scheduled by the operating system: Both threads and processes can be scheduled by the operating system to execute on the CPU. The operating system is responsible for assigning CPU time to the threads and processes based on various scheduling algorithms.\n5.Both have their own execution context: Each thread and process has its own execution context, which includes its own register set, program counter, and stack. This allows each thread or process to execute independently and make progress without interfering with other threads or processes. \n6.Both can communicate with each other: Threads and processes can communicate with each other using various inter-process communication (IPC) mechanisms such as shared memory, message queues, and pipes. This allows threads and processes to share data and coordinate their activities.\n7.Both can be preempted: Threads and processes can be preempted by the operating system, which means that their execution can be interrupted at any time. This allows the operating system to switch to another thread or process that needs to execute.\n8.Both can be terminated: Threads and processes can be terminated by the operating system or by other threads or processes. When a thread or process is terminated, all of its resources, including its execution context, are freed up and made available to other threads or processes.\n\nDifferences between Threads and Processes –  \n1.Resources: Processes have their own address space and resources, such as memory and file handles, whereas threads share memory and resources with the program that created them.\n2.Scheduling: Processes are scheduled to use the processor by the operating system, whereas threads are scheduled to use the processor by the operating system or the program itself.\n3.Creation: The operating system creates and manages processes, whereas the program or the operating system creates and manages threads.\n4.Communication: Because processes are isolated from one another and must rely on inter-process communication mechanisms, they generally have more difficulty communicating with one another than threads do. Threads, on the other hand, can interact with other threads within the same programme directly.\nThreads, in general, are lighter than processes and are better suited for concurrent execution within a single programme. Processes are commonly used to run separate programmes or to isolate resources between programmes."]]}],
        "main_memory":[{"text":[["The term memory can be defined as a collection of data in a specific format. It is used to store instructions and process data. The memory comprises a large array or group of words or bytes, each with its own location. The primary purpose of a computer system is to execute programs. These programs, along with the information they access, should be in the main memory during execution. The CPU fetches instructions from memory according to the value of the program counter.\n\nTo achieve a degree of multiprogramming and proper utilization of memory, memory management is important. Many memory management methods exist, reflecting various approaches, and the effectiveness of each algorithm depends on the situation.\n\nWhat is Main Memory?\nThe main memory is central to the operation of a Modern Computer. Main Memory is a large array of words or bytes, ranging in size from hundreds of thousands to billions. Main memory is a repository of rapidly available information shared by the CPU and I/O devices. Main memory is the place where programs and information are kept when the processor is effectively utilizing them.  Main memory is associated with the processor, so moving instructions and information into and out of the processor is extremely fast.  Main memory is also known as RAM (Random Access Memory). This memory is volatile. RAM loses its data when a power interruption occurs.\n\nWhat is Memory Management?\nIn a multiprogramming computer, the Operating System resides in a part of memory, and the rest is used by multiple processes. The task of subdividing the memory among different processes is called Memory Management. Memory management is a method in the operating system to manage operations between main memory and disk during process execution. The main aim of memory management is to achieve efficient utilization of memory.  \n\nWhy Memory Management is Required?\n1.Allocate and de-allocate memory before and after process execution.\n2.To keep track of used memory space by processes.\n3.To minimize fragmentation issues.\n4.To proper utilization of main memory.\n5.To maintain data integrity while executing of process.\n\nNow we are discussing the concept of Logical Address Space and Physical Address Space\nLogical and Physical Address Space\nLogical Address Space: An address generated by the CPU is known as a “Logical Address”. It is also known as a Virtual address. Logical address space can be defined as the size of the process. A logical address can be changed.\nPhysical Address Space: An address seen by the memory unit (i.e the one loaded into the memory address register of the memory) is commonly known as a “Physical Address”. A Physical address is also known as a Real address. The set of all physical addresses corresponding to these logical addresses is known as Physical address space. A physical address is computed by MMU. The run-time mapping from virtual to physical addresses is done by a hardware device Memory Management Unit(MMU). The Physical address always remains constant"],
        ["Static and Dynamic Loading\n\nLoading a process into the main memory is done by a loader.\n\nThere are two different types of loading :\n\nStatic Loading: Static Loading is basically loading the entire program into a fixed address. It requires more memory space.\n\nDynamic Loading: The entire program and all data of a process must be in physical memory for the process to execute. So, the size of a process is limited to the size of physical memory. To gain proper memory utilization, dynamic loading is used. In dynamic loading, a routine is not loaded until it is called. All routines are residing on disk in a relocatable load format. One of the advantages of dynamic loading is that the unused routine is never loaded. This loading is useful when a large amount of code is needed to handle it efficiently.\n\nStatic and Dynamic Linking\nTo perform a linking task a linker is used. A linker is a program that takes one or more object files generated by a compiler and combines them into a single executable file. \nStatic Linking: In static linking, the linker combines all necessary program modules into a single executable program. So there is no runtime dependency. Some operating systems support only static linking, in which system language libraries are treated like any other object module.\nDynamic Linking: The basic concept of dynamic linking is similar to dynamic loading. In dynamic linking, “Stub” is included for each appropriate library routine reference. A stub is a small piece of code. When the stub is executed, it checks whether the needed routine is already in memory or not. If not available then the program loads the routine into memory.\n\nSwapping\nWhen a process is executed it must have resided in memory. Swapping is a process of swapping a process temporarily into a secondary memory from the main memory, which is fast compared to secondary memory. A swapping allows more processes to be run and can be fit into memory at one time. The main part of swapping is transferred time and the total time is directly proportional to the amount of memory swapped. Swapping is also known as roll-out, or roll because if a higher priority process arrives and wants service, the memory manager can swap out the lower priority process and then load and execute the higher priority process. After finishing higher priority work, the lower priority process swapped back in memory and continued to the execution process.  "],
        ["Memory Management with Monoprogramming (Without Swapping)\n\nThis is the simplest memory management approach the memory is divided into two sections:\nOne part of the operating system\nThe second part of the user program\nIn this approach, the operating system keeps track of the first and last location available for the allocation of the user program\nThe operating system is loaded either at the bottom or at top\nInterrupt vectors are often loaded in low memory therefore, it makes sense to load the operating system in low memory\nSharing of data and code does not make much sense in a single process environment\nThe Operating system can be protected from user programs with the help of a fence register.\n\nAdvantages of Memory Management\nIt is a simple management approach\n\nDisadvantages of Memory Management\nIt does not support multiprogramming\nMemory is wasted\n\nMultiprogramming with Fixed Partitions (Without Swapping)\nA memory partition scheme with a fixed number of partitions was introduced to support multiprogramming. this scheme is based on contiguous allocation\nEach partition is a block of contiguous memory\nMemory is partitioned into a fixed number of partitions.\nEach partition is of fixed size\n\nLogical vs Physical Address\nAn address generated by the CPU is commonly referred to as a logical address. the address seen by the memory unit is known as the physical address. The logical address can be mapped to a physical address by hardware with the help of a base register this is known as dynamic relocation of memory references."],
        ["Contiguous  Memory Allocation\n\nThe main memory should accommodate both the operating system and the different client processes.  Therefore, the allocation of memory becomes an important task in the operating system.  The memory is usually divided into two partitions: one for the resident operating system and one for the user processes. We normally need several user processes to reside in memory simultaneously. Therefore, we need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory. In adjacent memory allotment, each process is contained in a single contiguous segment of memory.\n\nMemory Allocation\nTo gain proper memory utilization, memory allocation must be allocated efficient manner. One of the simplest methods for allocating memory is to divide memory into several fixed-sized partitions and each partition contains exactly one process. Thus, the degree of multiprogramming is obtained by the number of partitions. \n\nMultiple partition allocation: In this method, a process is selected from the input queue and loaded into the free partition. When the process terminates, the partition becomes available for other processes. \n\nFixed partition allocation: In this method, the operating system maintains a table that indicates which parts of memory are available and which are occupied by processes. Initially, all memory is available for user processes and is considered one large block of available memory. This available memory is known as a “Hole”. When the process arrives and needs memory, we search for a hole that is large enough to store this process. If the requirement is fulfilled then we allocate memory to process, otherwise keeping the rest available to satisfy future requests. While allocating a memory sometimes dynamic storage allocation problems occur, which concerns how to satisfy a request of size n from a list of free holes. There are some solutions to this problem:\n\nFirst Fit\nIn the First Fit, the first available free hole fulfil the requirement of the process allocated.\n\nBest Fit\nIn the Best Fit, allocate the smallest hole that is big enough to process requirements. For this, we search the entire list, unless the list is ordered by size.\n\nWorst Fit \nIn the Worst Fit, allocate the largest available hole to process. This method produces the largest leftover hole. "],
        ["Fragmentation\n\nFragmentation is defined as when the process is loaded and removed after execution from memory, it creates a small free hole. These holes can not be assigned to new processes because holes are not combined or do not fulfill the memory requirement of the process.  To achieve a degree of multiprogramming, we must reduce the waste of memory or fragmentation problems. In the operating systems two types of fragmentation:\nInternal fragmentation: Internal fragmentation occurs when memory blocks are allocated to the process more than their requested size. Due to this some unused space is left over and creating an internal fragmentation problem.Example: Suppose there is a fixed partitioning used for memory allocation and the different sizes of blocks 3MB, 6MB, and 7MB space in memory. Now a new process p4 of size 2MB comes and demands a block of memory. It gets a memory block of 3MB but 1MB block of memory is a waste, and it can not be allocated to other processes too. This is called internal fragmentation.\nExternal fragmentation: In External Fragmentation, we have a free memory block, but we can not assign it to a process because blocks are not contiguous. Example: Suppose (consider the above example) three processes p1, p2, and p3 come with sizes 2MB, 4MB, and 7MB respectively. Now they get memory blocks of size 3MB, 6MB, and 7MB allocated respectively. After allocating the process p1 process and the p2 process left 1MB and 2MB. Suppose a new process p4 comes and demands a 3MB block of memory, which is available, but we can not assign it because free memory space is not contiguous.  This is called external fragmentation.\nBoth the first-fit and best-fit systems for memory allocation are affected by external fragmentation. To overcome the external fragmentation problem Compaction is used. In the compaction technique, all free memory space combines and makes one large block. So, this space can be used by other processes effectively.\n\nAnother possible solution to the external fragmentation is to allow the logical address space of the processes to be noncontiguous, thus permitting a process to be allocated physical memory wherever the latter is available.\n\nPaging\nPaging is a memory management scheme that eliminates the need for a contiguous allocation of physical memory. This scheme permits the physical address space of a process to be non-contiguous.\nLogical Address or Virtual Address (represented in bits): An address generated by the CPU.\nLogical Address Space or Virtual Address Space (represented in words or bytes): The set of all logical addresses generated by a program.\nPhysical Address (represented in bits): An address actually available on a memory unit.\nPhysical Address Space (represented in words or bytes): The set of all physical addresses corresponding to the logical addresses."]]}],
        "disk":[{"text":[["data not available"]]}],
            "misc":[{"text":[["data not available"]]}]

    },
    {
        "rdbms":[{"text":[["data not available"]]}],
        "constraints":[{"text":[["data not available"]]}],
        "ddl":[{"text":[["data not available"]]}],
        "templates":[{"text":[["data not available"]]}],
        "tcl":[{"text":[["data not available"]]}],
        "keys":[{"text":[["data not available"]]}],
        "dml":[{"text":[["data not available"]]}]
    },
    {
        "closure":[{"text":[["data not available"]]}],
        "object":[{"text":[["data not available"]]}],
        "datatype":[{"text":[["data not available"]]}],
        "function":[{"text":[["data not available"]]}],
        "nan":[{"text":[["data not available"]]}],
        "asynchronous":[{"text":[["data not available"]]}]

    },
    {
        "array":[{"text":[["data not available"]]}],
        "class":[{"text":[["data not available"]]}],
        "abstract":[{"text":[["data not available"]]}],
        "datatype":[{"text":[["data not available"]]}],
        "objects":[{"text":[["data not available"]]}],
        "polymorphism":[{"text":[["data not available"]]}],
        "inheritance":[{"text":[["data not available"]]}],
        "string":[{"text":[["data not available"]]}],
        "thread":[{"text":[["data not available"]]}],
        "access_specifier":[{"text":[["data not available"]]}]

    },
    {
        "component":[{"text":[["data not available"]]}],
        "state":[{"text":[["data not available"]]}],
        "hooks":[{"text":[["data not available"]]}],
        "class":[{"text":[["data not available"]]}],
        "props":[{"text":[["data not available"]]}],
        "callbacks":[{"text":[["data not available"]]}]

    }
]




